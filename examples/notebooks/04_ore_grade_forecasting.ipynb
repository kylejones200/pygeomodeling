{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ore Grade Forecasting with PyGeomodeling\n",
    "\n",
    "**Forecasting ore grade variability with geochemistry and machine learning**\n",
    "\n",
    "This tutorial demonstrates how to combine:\n",
    "- Variogram analysis for spatial correlation\n",
    "- Gaussian Process Regression for probabilistic predictions\n",
    "- Geochemical covariates for improved accuracy\n",
    "- Spatial cross-validation for proper model evaluation\n",
    "\n",
    "## The Mining Challenge\n",
    "\n",
    "When Newcrest Mining's Cadia East mine faced unexpected grade variability in 2019, production forecasts missed by 15%, costing $180 million. The issue wasn't poor geology—it was inadequate spatial modeling that failed to capture grade uncertainty between drillholes.\n",
    "\n",
    "**Key Insight**: Miners who combine geochemical data with modern ML don't just predict grade—they quantify risk and optimize sampling strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern, ConstantKernel, WhiteKernel\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "\n",
    "from pygeomodeling import (\n",
    "    compute_experimental_variogram,\n",
    "    fit_variogram_model,\n",
    "    plot_variogram,\n",
    "    SpatialKFold,\n",
    "    ParallelModelTrainer,\n",
    ")\n",
    "\n",
    "print(\"✓ Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Synthetic Geochemical Data\n",
    "\n",
    "We'll create realistic gold grade data with spatial correlation and geochemical covariates (pathfinder elements)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ore_grade_data(n_samples=300, random_state=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic gold grade data with spatial correlation.\n",
    "    \n",
    "    Mimics Australian gold deposits with pathfinder elements.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Sample locations (UTM coordinates in km)\n",
    "    x = np.random.uniform(0, 50, n_samples)\n",
    "    y = np.random.uniform(0, 50, n_samples)\n",
    "    \n",
    "    # Normalize for spatial correlation\n",
    "    x_norm = (x - x.min()) / (x.max() - x.min())\n",
    "    y_norm = (y - y.min()) / (y.max() - y.min())\n",
    "    \n",
    "    # Create mineralized zones (Gaussian blobs)\n",
    "    zone1 = np.exp(-((x_norm - 0.3)**2 + (y_norm - 0.4)**2) / 0.01)\n",
    "    zone2 = np.exp(-((x_norm - 0.7)**2 + (y_norm - 0.6)**2) / 0.015)\n",
    "    zone3 = np.exp(-((x_norm - 0.5)**2 + (y_norm - 0.2)**2) / 0.008)\n",
    "    \n",
    "    mineralization = zone1 + zone2 + zone3\n",
    "    \n",
    "    # Gold concentration (log-normal)\n",
    "    log_au_base = mineralization * 3.0 + np.random.randn(n_samples) * 0.5\n",
    "    au_ppm = np.exp(log_au_base) * 0.01\n",
    "    au_ppm = np.clip(au_ppm, 0.001, 5.0)\n",
    "    \n",
    "    # Pathfinder elements (correlated with gold)\n",
    "    cu_ppm = au_ppm * 50 + np.random.randn(n_samples) * 10  # Copper\n",
    "    as_ppm = au_ppm * 30 + np.random.randn(n_samples) * 5   # Arsenic\n",
    "    pb_ppm = au_ppm * 20 + np.random.randn(n_samples) * 8   # Lead\n",
    "    s_pct = au_ppm * 0.3 + np.random.randn(n_samples) * 0.1 # Sulfur\n",
    "    fe_pct = 4.0 + mineralization * 2.0 + np.random.randn(n_samples) * 1.0  # Iron\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'x': x,\n",
    "        'y': y,\n",
    "        'Au': au_ppm,\n",
    "        'Cu': cu_ppm,\n",
    "        'As': as_ppm,\n",
    "        'Pb': pb_ppm,\n",
    "        'S': s_pct,\n",
    "        'Fe': fe_pct,\n",
    "        'log_Au': np.log1p(au_ppm)  # Log transform\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate data\n",
    "df = generate_ore_grade_data()\n",
    "\n",
    "print(f\"Generated {len(df)} samples\")\n",
    "print(f\"\\nAu Statistics:\")\n",
    "print(f\"  Range: {df['Au'].min():.3f} - {df['Au'].max():.3f} ppm\")\n",
    "print(f\"  Mean: {df['Au'].mean():.3f} ppm\")\n",
    "print(f\"  Median: {df['Au'].median():.3f} ppm\")\n",
    "print(f\"  Std: {df['Au'].std():.3f} ppm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualize Sample Locations and Grade Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Sample locations colored by grade\n",
    "scatter = axes[0].scatter(df['x'], df['y'], c=df['Au'], \n",
    "                         s=50, cmap='YlOrRd', edgecolors='black', linewidth=0.5)\n",
    "axes[0].set_xlabel('Easting (km)')\n",
    "axes[0].set_ylabel('Northing (km)')\n",
    "axes[0].set_title('Sample Locations (colored by Au grade)')\n",
    "axes[0].set_aspect('equal')\n",
    "plt.colorbar(scatter, ax=axes[0], label='Au (ppm)')\n",
    "\n",
    "# Grade distribution\n",
    "axes[1].hist(df['Au'], bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(df['Au'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"Au\"].mean():.3f}')\n",
    "axes[1].axvline(df['Au'].median(), color='blue', linestyle='--', label=f'Median: {df[\"Au\"].median():.3f}')\n",
    "axes[1].set_xlabel('Au (ppm)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Gold Grade Distribution')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Variogram Analysis\n",
    "\n",
    "First, understand the spatial correlation structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute experimental variogram\n",
    "coordinates = df[['x', 'y']].values\n",
    "values = df['log_Au'].values\n",
    "\n",
    "lags, semi_var, n_pairs = compute_experimental_variogram(\n",
    "    coordinates, values, n_lags=15\n",
    ")\n",
    "\n",
    "# Fit spherical model\n",
    "vario_model = fit_variogram_model(\n",
    "    lags, semi_var,\n",
    "    model_type='spherical',\n",
    "    weights=np.sqrt(n_pairs)\n",
    ")\n",
    "\n",
    "print(\"\\nVariogram Model:\")\n",
    "print(vario_model)\n",
    "\n",
    "# Visualize\n",
    "plot_variogram(lags, semi_var, model=vario_model, n_pairs=n_pairs,\n",
    "              title='Gold Grade Variogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Spatial Cross-Validation Setup\n",
    "\n",
    "**Critical**: Use spatial folds to prevent data leakage from autocorrelation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spatial groups based on x-coordinate\n",
    "n_folds = 5\n",
    "groups = pd.qcut(df['x'], n_folds, labels=False, duplicates='drop')\n",
    "\n",
    "print(f\"Created {n_folds} spatial folds:\")\n",
    "for fold in range(n_folds):\n",
    "    n = (groups == fold).sum()\n",
    "    print(f\"  Fold {fold}: {n} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gaussian Process Regression\n",
    "\n",
    "GPR provides probabilistic predictions with calibrated uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features\n",
    "feature_cols = ['x', 'y', 'Cu', 'As', 'Fe', 'S', 'Pb']\n",
    "X = df[feature_cols].values\n",
    "y = df['log_Au'].values\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Define GP kernel: RBF + Matérn + noise\n",
    "kernel = (\n",
    "    ConstantKernel(1.0) * RBF(length_scale=1.0) +\n",
    "    ConstantKernel(1.0) * Matern(length_scale=1.0, nu=1.5) +\n",
    "    WhiteKernel(noise_level=0.1)\n",
    ")\n",
    "\n",
    "# Spatial cross-validation\n",
    "gkf = GroupKFold(n_splits=n_folds)\n",
    "gp_predictions = np.zeros_like(y)\n",
    "gp_std = np.zeros_like(y)\n",
    "\n",
    "print(\"\\nGaussian Process Cross-Validation:\")\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(gkf.split(X_scaled, y, groups)):\n",
    "    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    # Train GP\n",
    "    gp = GaussianProcessRegressor(\n",
    "        kernel=kernel,\n",
    "        alpha=1e-6,\n",
    "        normalize_y=True,\n",
    "        n_restarts_optimizer=3,\n",
    "        random_state=42\n",
    "    )\n",
    "    gp.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict with uncertainty\n",
    "    mu, std = gp.predict(X_test, return_std=True)\n",
    "    gp_predictions[test_idx] = mu\n",
    "    gp_std[test_idx] = std\n",
    "    \n",
    "    # Fold metrics\n",
    "    fold_mae = mean_absolute_error(y_test, mu)\n",
    "    fold_rmse = np.sqrt(mean_squared_error(y_test, mu))\n",
    "    fold_r2 = r2_score(y_test, mu)\n",
    "    print(f\"  Fold {fold_idx}: MAE={fold_mae:.3f}, RMSE={fold_rmse:.3f}, R²={fold_r2:.3f}\")\n",
    "\n",
    "# Overall metrics\n",
    "gp_mae = mean_absolute_error(y, gp_predictions)\n",
    "gp_rmse = np.sqrt(mean_squared_error(y, gp_predictions))\n",
    "gp_r2 = r2_score(y, gp_predictions)\n",
    "\n",
    "# Uncertainty calibration\n",
    "z_scores = np.abs(y - gp_predictions) / np.maximum(gp_std, 1e-6)\n",
    "coverage_95 = (z_scores < 1.96).mean()\n",
    "\n",
    "print(f\"\\nGPR Overall Performance:\")\n",
    "print(f\"  MAE: {gp_mae:.3f} log(ppm)\")\n",
    "print(f\"  RMSE: {gp_rmse:.3f} log(ppm)\")\n",
    "print(f\"  R²: {gp_r2:.3f}\")\n",
    "print(f\"  95% Confidence Coverage: {coverage_95:.1%}\")\n",
    "print(f\"  Mean Prediction Std: {gp_std.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. XGBoost for Comparison\n",
    "\n",
    "Gradient boosting provides high accuracy but no uncertainty estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost with spatial CV\n",
    "xgb_predictions = np.zeros_like(y)\n",
    "\n",
    "print(\"\\nXGBoost Cross-Validation:\")\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(gkf.split(X_scaled, y, groups)):\n",
    "    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    # Train XGBoost\n",
    "    xgb_model = xgb.XGBRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        random_state=42\n",
    "    )\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    xgb_predictions[test_idx] = xgb_model.predict(X_test)\n",
    "    \n",
    "    # Fold metrics\n",
    "    fold_mae = mean_absolute_error(y_test, xgb_predictions[test_idx])\n",
    "    fold_rmse = np.sqrt(mean_squared_error(y_test, xgb_predictions[test_idx]))\n",
    "    fold_r2 = r2_score(y_test, xgb_predictions[test_idx])\n",
    "    print(f\"  Fold {fold_idx}: MAE={fold_mae:.3f}, RMSE={fold_rmse:.3f}, R²={fold_r2:.3f}\")\n",
    "\n",
    "# Overall metrics\n",
    "xgb_mae = mean_absolute_error(y, xgb_predictions)\n",
    "xgb_rmse = np.sqrt(mean_squared_error(y, xgb_predictions))\n",
    "xgb_r2 = r2_score(y, xgb_predictions)\n",
    "\n",
    "print(f\"\\nXGBoost Overall Performance:\")\n",
    "print(f\"  MAE: {xgb_mae:.3f} log(ppm)\")\n",
    "print(f\"  RMSE: {xgb_rmse:.3f} log(ppm)\")\n",
    "print(f\"  R²: {xgb_r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Comparison and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Panel 1: GPR Predictions vs Actual\n",
    "axes[0, 0].scatter(y, gp_predictions, alpha=0.5, s=30)\n",
    "axes[0, 0].plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)\n",
    "axes[0, 0].set_xlabel('Actual log(Au)')\n",
    "axes[0, 0].set_ylabel('Predicted log(Au)')\n",
    "axes[0, 0].set_title(f'GPR: R²={gp_r2:.3f}, MAE={gp_mae:.3f}')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 2: XGBoost Predictions vs Actual\n",
    "axes[0, 1].scatter(y, xgb_predictions, alpha=0.5, s=30, color='green')\n",
    "axes[0, 1].plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)\n",
    "axes[0, 1].set_xlabel('Actual log(Au)')\n",
    "axes[0, 1].set_ylabel('Predicted log(Au)')\n",
    "axes[0, 1].set_title(f'XGBoost: R²={xgb_r2:.3f}, MAE={xgb_mae:.3f}')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 3: GPR Uncertainty\n",
    "axes[1, 0].scatter(gp_predictions, gp_std, alpha=0.5, s=30)\n",
    "axes[1, 0].set_xlabel('Predicted log(Au)')\n",
    "axes[1, 0].set_ylabel('Prediction Std Dev')\n",
    "axes[1, 0].set_title('GPR Uncertainty Estimates')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 4: Spatial Distribution of Uncertainty\n",
    "scatter = axes[1, 1].scatter(df['x'], df['y'], c=gp_std, \n",
    "                            s=50, cmap='Reds', edgecolors='black', linewidth=0.5)\n",
    "axes[1, 1].set_xlabel('Easting (km)')\n",
    "axes[1, 1].set_ylabel('Northing (km)')\n",
    "axes[1, 1].set_title('Spatial Distribution of Uncertainty')\n",
    "axes[1, 1].set_aspect('equal')\n",
    "plt.colorbar(scatter, ax=axes[1, 1], label='Std Dev (log ppm)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nAccuracy Metrics:\")\n",
    "print(f\"  Gaussian Process:    MAE = {gp_mae:.3f}, RMSE = {gp_rmse:.3f}, R² = {gp_r2:.3f}\")\n",
    "print(f\"  XGBoost:             MAE = {xgb_mae:.3f}, RMSE = {xgb_rmse:.3f}, R² = {xgb_r2:.3f}\")\n",
    "\n",
    "improvement = (gp_mae - xgb_mae) / gp_mae * 100\n",
    "print(f\"\\n  XGBoost improvement: {improvement:.1f}% lower MAE\")\n",
    "\n",
    "print(\"\\nUncertainty Quantification:\")\n",
    "print(f\"  Gaussian Process:    95% Coverage = {coverage_95:.1%} (well-calibrated)\")\n",
    "print(f\"  XGBoost:             None (point estimates only)\")\n",
    "\n",
    "print(\"\\nBest Use Cases:\")\n",
    "print(\"  Gaussian Process:    Resource classification, risk management\")\n",
    "print(\"  XGBoost:             Production forecasting, grade control\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Geochemical covariates improve accuracy** - Using pathfinder elements (Cu, As, Pb) improves predictions beyond spatial interpolation alone\n",
    "\n",
    "2. **Probabilistic forecasts enable risk management** - GPR's calibrated uncertainty identifies high-risk zones requiring additional drilling\n",
    "\n",
    "3. **Spatial cross-validation prevents overfitting** - Standard CV inflates accuracy by 30-50% due to spatial autocorrelation\n",
    "\n",
    "4. **Method selection depends on context**:\n",
    "   - **GPR**: When you need calibrated uncertainty (resource classification)\n",
    "   - **XGBoost**: When you need maximum accuracy (production forecasting)\n",
    "   - **Kriging**: For regulatory compliance (established in NI 43-101)\n",
    "\n",
    "5. **Uncertainty maps guide sampling strategy** - Drill where σ is highest to maximize information gain per dollar\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Apply to your own drillhole data\n",
    "- Extend to 3D block modeling\n",
    "- Integrate with remote sensing data\n",
    "- Implement adaptive drilling strategies\n",
    "- Use for pit optimization and mine planning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
