{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddfabe4a-9e3e-4f6a-a005-1b45f35f7d57",
   "metadata": {},
   "source": [
    "Petrophysics Evaluation: Lithology Prediction From Well Logs Using Deep Learning\n",
    "Overview\n",
    "This project demonstrates an advanced approach for well log facies classification using deep learning on AWS SageMaker. We'll use a modern ML pipeline to predict lithology from well log measurements, leveraging the latest AWS services and best practices.\n",
    "\n",
    "Updated reference architecture for petrophysics and well log analytics\n",
    "\n",
    "Contents\n",
    "[Setup](https://console.harmony.a2z.com/internal-ai-assistant#setup)\n",
    "[Data Preparation](https://console.harmony.a2z.com/internal-ai-assistant#data-preparation)\n",
    "[Model Development](https://console.harmony.a2z.com/internal-ai-assistant#model-development)\n",
    "[Training and Tuning](https://console.harmony.a2z.com/internal-ai-assistant#training-and-tuning)\n",
    "[Deployment and Inference](https://console.harmony.a2z.com/internal-ai-assistant#deployment-and-inference)\n",
    "[Monitoring and MLOps](https://console.harmony.a2z.com/internal-ai-assistant#monitoring-and-mlops)\n",
    "[Cleanup](https://console.harmony.a2z.com/internal-ai-assistant#cleanup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9fb1ff92-a479-4d11-aa0e-e470c9cdd368",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Setup\n",
    "First, let's set up our environment and configure AWS credentials:\n",
    "\"\"\"  \n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "prefix = 'sagemaker/petrophysics-facies-classification'\n",
    "\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68165f7a-a600-4cf6-9fb7-7f95af7c5276",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'awswrangler'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mData Preparation\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mWe'll use AWS Glue and Amazon Athena to prepare and query our data:\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mawswrangler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwr\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Assuming data is already in S3\u001b[39;00m\n\u001b[1;32m      9\u001b[0m s3_input_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbucket\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/parquet_data/welllog_data.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'awswrangler'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data Preparation\n",
    "We'll use AWS Glue and Amazon Athena to prepare and query our data:\n",
    "\"\"\"\n",
    "    \n",
    "import awswrangler as wr\n",
    "\n",
    "# Assuming data is already in S3\n",
    "s3_input_path = f\"s3://{bucket}/{prefix}/parquet_data/welllog_data.parquet\"\n",
    "\n",
    "# Create the Glue database if it doesn't exist\n",
    "wr.catalog.create_database(\n",
    "    name=\"petrophysics_db\",\n",
    "    description=\"Database for petrophysics data\",\n",
    "    exist_ok=True  # This will not raise an error if the database already exists\n",
    ")\n",
    "\n",
    "# Create Glue catalog table\n",
    "wr.catalog.create_parquet_table(\n",
    "    database=\"petrophysics_db\",\n",
    "    table=\"well_logs\",\n",
    "    path=s3_input_path,\n",
    "    columns_types={\n",
    "        \"Well Name\": \"string\",\n",
    "        \"Depth\": \"float\",\n",
    "        \"GR\": \"float\",\n",
    "        \"ILD_log10\": \"float\",\n",
    "        \"DeltaPHI\": \"float\",\n",
    "        \"PHIND\": \"float\",\n",
    "        \"PE\": \"float\",\n",
    "        \"NM_M\": \"int\",\n",
    "        \"RELPOS\": \"float\",\n",
    "        \"Facies\": \"string\"\n",
    "    },\n",
    "    partitions_types={},\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21e5d26-dfb1-4d0b-be45-c805fefb7232",
   "metadata": {},
   "outputs": [
    {
     "ename": "QueryFailed",
     "evalue": "COLUMN_NOT_FOUND: line 8:13: Column 'well name' cannot be resolved or requester is not authorized to access requested resources. You may need to manually clean the data at location 's3://aws-athena-query-results-479837311121-us-east-1/tables/9542f326-9e80-4427-96d9-ef2adb2845fc' before retrying. Athena will not delete data in your account.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mQueryFailed\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Query data using Athena\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mwr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mathena\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sql_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43msql\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;43m    SELECT * FROM petrophysics_db.well_logs\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;43m    WHERE \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWell Name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m NOT IN (\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSHRIMPLIN\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSHANKLE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpetrophysics_db\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/awswrangler/_config.py:712\u001b[0m, in \u001b[0;36mapply_configs.<locals>.wrapper\u001b[0;34m(*args_raw, **kwargs)\u001b[0m\n\u001b[1;32m    710\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m args[name]\n\u001b[1;32m    711\u001b[0m         args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkeywords}\n\u001b[0;32m--> 712\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/awswrangler/_utils.py:179\u001b[0m, in \u001b[0;36mvalidate_kwargs.<locals>.decorator.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m condition_fn() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(passed_unsupported_kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mInvalidArgument(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(passed_unsupported_kwargs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/awswrangler/athena/_read.py:1083\u001b[0m, in \u001b[0;36mread_sql_query\u001b[0;34m(sql, database, ctas_approach, unload_approach, ctas_parameters, unload_parameters, categories, chunksize, s3_output, workgroup, encryption, kms_key, keep_files, use_threads, boto3_session, client_request_token, athena_cache_settings, data_source, athena_query_wait_polling_delay, params, paramstyle, dtype_backend, s3_additional_kwargs, pyarrow_additional_kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m ctas_bucketing_info \u001b[38;5;241m=\u001b[39m ctas_parameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbucketing_info\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1081\u001b[0m ctas_write_compression \u001b[38;5;241m=\u001b[39m ctas_parameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1083\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_resolve_query_without_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43msql\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_source\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctas_approach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctas_approach\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43munload_approach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munload_approach\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m    \u001b[49m\u001b[43munload_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munload_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[43m    \u001b[49m\u001b[43ms3_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms3_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencryption\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencryption\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkms_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkms_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctas_database\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctas_database\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctas_temp_table_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctas_temp_table_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctas_bucketing_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctas_bucketing_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctas_write_compression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctas_write_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mathena_query_wait_polling_delay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mathena_query_wait_polling_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[43ms3_additional_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms3_additional_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mboto3_session\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mboto3_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpyarrow_additional_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpyarrow_additional_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecution_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecution_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_request_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_request_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/awswrangler/athena/_read.py:508\u001b[0m, in \u001b[0;36m_resolve_query_without_cache\u001b[0;34m(sql, database, data_source, ctas_approach, unload_approach, unload_parameters, categories, chunksize, s3_output, workgroup, encryption, kms_key, keep_files, ctas_database, ctas_temp_table_name, ctas_bucketing_info, ctas_write_compression, athena_query_wait_polling_delay, use_threads, s3_additional_kwargs, boto3_session, pyarrow_additional_kwargs, execution_params, dtype_backend, client_request_token)\u001b[0m\n\u001b[1;32m    506\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemp_table_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muuid\u001b[38;5;241m.\u001b[39muuid4()\u001b[38;5;241m.\u001b[39mhex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_resolve_query_without_cache_ctas\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43msql\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_source\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43ms3_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms3_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcategories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencryption\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencryption\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworkgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkms_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkms_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43malt_database\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctas_database\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctas_bucketing_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctas_bucketing_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctas_write_compression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctas_write_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mathena_query_wait_polling_delay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mathena_query_wait_polling_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43ms3_additional_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms3_additional_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mboto3_session\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mboto3_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpyarrow_additional_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpyarrow_additional_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexecution_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecution_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m     catalog\u001b[38;5;241m.\u001b[39mdelete_table_if_exists(database\u001b[38;5;241m=\u001b[39mctas_database \u001b[38;5;129;01mor\u001b[39;00m database, table\u001b[38;5;241m=\u001b[39mname, boto3_session\u001b[38;5;241m=\u001b[39mboto3_session)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/awswrangler/athena/_read.py:325\u001b[0m, in \u001b[0;36m_resolve_query_without_cache_ctas\u001b[0;34m(sql, database, data_source, s3_output, keep_files, chunksize, categories, encryption, workgroup, kms_key, alt_database, name, ctas_bucketing_info, ctas_write_compression, athena_query_wait_polling_delay, use_threads, s3_additional_kwargs, boto3_session, pyarrow_additional_kwargs, execution_params, dtype_backend)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_resolve_query_without_cache_ctas\u001b[39m(\n\u001b[1;32m    303\u001b[0m     sql: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    304\u001b[0m     database: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    323\u001b[0m     dtype_backend: Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy_nullable\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy_nullable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    324\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame \u001b[38;5;241m|\u001b[39m Iterator[pd\u001b[38;5;241m.\u001b[39mDataFrame]:\n\u001b[0;32m--> 325\u001b[0m     ctas_query_info: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m _QueryMetadata] \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_ctas_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m        \u001b[49m\u001b[43msql\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctas_table\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctas_database\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malt_database\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbucketing_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctas_bucketing_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_source\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43ms3_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms3_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworkgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencryption\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencryption\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrite_compression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctas_write_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkms_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkms_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mathena_query_wait_polling_delay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mathena_query_wait_polling_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mboto3_session\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mboto3_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecution_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparamstyle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mqmark\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m     fully_qualified_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mctas_query_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mctas_database\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mctas_query_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mctas_table\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    344\u001b[0m     ctas_query_metadata \u001b[38;5;241m=\u001b[39m cast(_QueryMetadata, ctas_query_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mctas_query_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/awswrangler/_config.py:712\u001b[0m, in \u001b[0;36mapply_configs.<locals>.wrapper\u001b[0;34m(*args_raw, **kwargs)\u001b[0m\n\u001b[1;32m    710\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m args[name]\n\u001b[1;32m    711\u001b[0m         args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkeywords}\n\u001b[0;32m--> 712\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/awswrangler/athena/_utils.py:869\u001b[0m, in \u001b[0;36mcreate_ctas_table\u001b[0;34m(sql, database, ctas_table, ctas_database, s3_output, storage_format, write_compression, partitioning_info, bucketing_info, field_delimiter, schema_only, workgroup, data_source, encryption, kms_key, categories, wait, athena_query_wait_polling_delay, execution_params, params, paramstyle, boto3_session)\u001b[0m\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn type is unknown\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m msg:\n\u001b[1;32m    865\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mInvalidArgumentValue(\n\u001b[1;32m    866\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease, don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt leave undefined columns types in your query. You can cast to ensure it. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(E.g. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSELECT CAST(NULL AS INTEGER) AS MY_COL, ...\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    868\u001b[0m             )\n\u001b[0;32m--> 869\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ex\n\u001b[1;32m    870\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mctas_query_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m query_execution_id\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/awswrangler/athena/_utils.py:847\u001b[0m, in \u001b[0;36mcreate_ctas_table\u001b[0;34m(sql, database, ctas_table, ctas_database, s3_output, storage_format, write_compression, partitioning_info, bucketing_info, field_delimiter, schema_only, workgroup, data_source, encryption, kms_key, categories, wait, athena_query_wait_polling_delay, execution_params, params, paramstyle, boto3_session)\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[1;32m    846\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 847\u001b[0m         response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mctas_query_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43m_get_query_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m            \u001b[49m\u001b[43mquery_execution_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_execution_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m            \u001b[49m\u001b[43mboto3_session\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mboto3_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcategories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata_cache_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_cache_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m            \u001b[49m\u001b[43mathena_query_wait_polling_delay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mathena_query_wait_polling_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mQueryFailed \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m    855\u001b[0m         msg: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(ex)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/awswrangler/athena/_utils.py:233\u001b[0m, in \u001b[0;36m_get_query_metadata\u001b[0;34m(query_execution_id, boto3_session, categories, query_execution_payload, metadata_cache_manager, athena_query_wait_polling_delay, execution_params, dtype_backend)\u001b[0m\n\u001b[1;32m    229\u001b[0m     _query_execution_payload \u001b[38;5;241m=\u001b[39m query_execution_payload\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m     _query_execution_payload \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryExecutionTypeDef\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 233\u001b[0m         \u001b[43m_executions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m            \u001b[49m\u001b[43mquery_execution_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_execution_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m            \u001b[49m\u001b[43mboto3_session\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mboto3_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m            \u001b[49m\u001b[43mathena_query_wait_polling_delay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mathena_query_wait_polling_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    238\u001b[0m     )\n\u001b[1;32m    239\u001b[0m cols_types: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m get_query_columns_types(\n\u001b[1;32m    240\u001b[0m     query_execution_id\u001b[38;5;241m=\u001b[39mquery_execution_id, boto3_session\u001b[38;5;241m=\u001b[39mboto3_session\n\u001b[1;32m    241\u001b[0m )\n\u001b[1;32m    242\u001b[0m _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCasting query column types: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, cols_types)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/awswrangler/_config.py:712\u001b[0m, in \u001b[0;36mapply_configs.<locals>.wrapper\u001b[0;34m(*args_raw, **kwargs)\u001b[0m\n\u001b[1;32m    710\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m args[name]\n\u001b[1;32m    711\u001b[0m         args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkeywords}\n\u001b[0;32m--> 712\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/awswrangler/athena/_executions.py:230\u001b[0m, in \u001b[0;36mwait_query\u001b[0;34m(query_execution_id, boto3_session, athena_query_wait_polling_delay)\u001b[0m\n\u001b[1;32m    228\u001b[0m _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery state change reason: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatus\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStateChangeReason\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFAILED\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 230\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mQueryFailed(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatus\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStateChangeReason\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANCELLED\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mQueryCancelled(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatus\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStateChangeReason\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mQueryFailed\u001b[0m: COLUMN_NOT_FOUND: line 8:13: Column 'well name' cannot be resolved or requester is not authorized to access requested resources. You may need to manually clean the data at location 's3://aws-athena-query-results-479837311121-us-east-1/tables/9542f326-9e80-4427-96d9-ef2adb2845fc' before retrying. Athena will not delete data in your account."
     ]
    }
   ],
   "source": [
    "# Query data using Athena\n",
    "df = wr.athena.read_sql_query(\n",
    "    sql=\"\"\"\n",
    "    SELECT * FROM petrophysics_db.well_logs\n",
    "    WHERE \"Well Name\" NOT IN ('SHRIMPLIN', 'SHANKLE')\n",
    "    \"\"\",\n",
    "    database=\"petrophysics_db\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383c41df-eca6-4838-ab4d-9e566cd91e03",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['depth', 'gr', 'ild_log10', 'deltaphi', 'phind', 'pe', 'nm_m', 'relpos',\\n       'facies'],\\n      dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mild_log10\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeltaphi\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphind\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpe\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnm_m\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelpos\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfacies\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfacies\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 6\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      7\u001b[0m y \u001b[38;5;241m=\u001b[39m df[target]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Split data\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[1;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[0;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['depth', 'gr', 'ild_log10', 'deltaphi', 'phind', 'pe', 'nm_m', 'relpos',\\n       'facies'],\\n      dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare features and target\n",
    "features = ['depth', 'gr', 'ild_log10', 'deltaphi', 'phind', 'pe',\n",
    "       'nm_m', 'relpos', 'facies']\n",
    "target = 'facies'\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save processed data to S3\n",
    "wr.s3.to_parquet(\n",
    "    df=pd.concat([X_train, y_train], axis=1),\n",
    "    path=f\"s3://{bucket}/{prefix}/processed_data/train/\",\n",
    "    dataset=True\n",
    ")\n",
    "\n",
    "wr.s3.to_parquet(\n",
    "    df=pd.concat([X_test, y_test], axis=1),\n",
    "    path=f\"s3://{bucket}/{prefix}/processed_data/test/\",\n",
    "    dataset=True\n",
    ")\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006e0ba8-a9ab-4c1a-aeca-aafe2599de4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape: (4149, 11)\n",
      "\n",
      "Dataframe info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4149 entries, 0 to 4148\n",
      "Data columns (total 11 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   Facies     4149 non-null   object \n",
      " 1   Formation  4149 non-null   object \n",
      " 2   Well Name  4149 non-null   object \n",
      " 3   Depth      4149 non-null   float64\n",
      " 4   GR         4149 non-null   float64\n",
      " 5   ILD_log10  4149 non-null   float64\n",
      " 6   DeltaPHI   4149 non-null   float64\n",
      " 7   PHIND      4149 non-null   float64\n",
      " 8   PE         3232 non-null   float64\n",
      " 9   NM_M       4149 non-null   int64  \n",
      " 10  RELPOS     4149 non-null   float64\n",
      "dtypes: float64(7), int64(1), object(3)\n",
      "memory usage: 356.7+ KB\n",
      "\n",
      "Parquet file saved to: s3://sagemaker-us-east-1-479837311121/sagemaker/petrophysics-facies-classification/parquet_data/welllog_data.parquet\n",
      "Error verifying file upload: An error occurred (404) when calling the HeadObject operation: Not Found\n",
      "\n",
      "Sample from uploaded Parquet file:\n",
      "                Facies Formation Well_Name   Depth     GR  ILD_log10  \\\n",
      "0  Nonmarine sandstone     A1 SH   SHANKLE  2785.0  58.63      0.444   \n",
      "1  Nonmarine sandstone     A1 SH   SHANKLE  2785.5  59.16      0.413   \n",
      "2  Nonmarine sandstone     A1 SH   SHANKLE  2786.0  52.86      0.378   \n",
      "3  Nonmarine sandstone     A1 SH   SHANKLE  2786.5  54.32      0.344   \n",
      "4  Nonmarine sandstone     A1 SH   SHANKLE  2787.0  53.27      0.320   \n",
      "\n",
      "   DeltaPHI   PHIND   PE  NM_M  RELPOS  \n",
      "0       4.4  12.440  2.7     1   0.661  \n",
      "1       3.7  13.640  2.6     1   0.645  \n",
      "2       2.5  14.490  2.6     1   0.629  \n",
      "3       1.1  15.085  2.6     1   0.613  \n",
      "4      -0.3  14.985  2.7     1   0.597  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import awswrangler as wr\n",
    "import boto3\n",
    "\n",
    "role = get_execution_role()\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "prefix = 'sagemaker/petrophysics-facies-classification'\n",
    "\n",
    "# Path to your local CSV file\n",
    "local_csv_path = \"facies_vectors.csv\"  # Replace with your local file path\n",
    "\n",
    "# S3 path for the Parquet file\n",
    "s3_parquet_path = f\"s3://{bucket}/{prefix}/parquet_data/welllog_data.parquet\"\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(local_csv_path)\n",
    "\n",
    "# Print info about the dataframe\n",
    "print(f\"Dataframe shape: {df.shape}\")\n",
    "print(\"\\nDataframe info:\")\n",
    "df.info()\n",
    "\n",
    "# Convert to Parquet and save to S3\n",
    "wr.s3.to_parquet(\n",
    "    df=df,\n",
    "    path=s3_parquet_path,\n",
    "    index=False,\n",
    "    dataset=True\n",
    ")\n",
    "\n",
    "print(f\"\\nParquet file saved to: {s3_parquet_path}\")\n",
    "\n",
    "# Verify the file was uploaded\n",
    "try:\n",
    "    obj = s3.head_object(Bucket=bucket, Key=f\"{prefix}/parquet_data/welllog_data.parquet\")\n",
    "    print(f\"File successfully uploaded. Size: {obj['ContentLength']} bytes\")\n",
    "except Exception as e:\n",
    "    print(f\"Error verifying file upload: {str(e)}\")\n",
    "\n",
    "# Optional: Read back a sample from the Parquet file to verify\n",
    "df_sample = wr.s3.read_parquet(path=s3_parquet_path, dataset=True).head()\n",
    "print(\"\\nSample from uploaded Parquet file:\")\n",
    "print(df_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eb561d-723c-49e6-b3fb-b73f0d6bc9d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "QueryFailed",
     "evalue": "COLUMN_NOT_FOUND: line 8:13: Column 'well name' cannot be resolved or requester is not authorized to access requested resources. You may need to manually clean the data at location 's3://aws-athena-query-results-479837311121-us-east-1/tables/380e8c7b-645b-4c5a-8282-18d38e2bfac9' before retrying. Athena will not delete data in your account.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mQueryFailed\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 40\u001b[0m\n\u001b[1;32m     19\u001b[0m wr\u001b[38;5;241m.\u001b[39mcatalog\u001b[38;5;241m.\u001b[39mcreate_csv_table(\n\u001b[1;32m     20\u001b[0m     database\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpetrophysics_db\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     21\u001b[0m     table\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwell_logs\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     partitions_types\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Query data using Athena\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mwr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mathena\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sql_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43msql\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;43m    SELECT * FROM petrophysics_db.well_logs\u001b[39;49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;43m    WHERE \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWell Name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m NOT IN (\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSHRIMPLIN\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSHANKLE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;43m    \u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpetrophysics_db\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     46\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Prepare features and target\u001b[39;00m\n\u001b[1;32m     49\u001b[0m features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGR\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mILD_log10\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDeltaPHI\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPHIND\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPE\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNM_M\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRELPOS\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/awswrangler/_config.py:712\u001b[0m, in \u001b[0;36mapply_configs.<locals>.wrapper\u001b[0;34m(*args_raw, **kwargs)\u001b[0m\n\u001b[1;32m    710\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m args[name]\n\u001b[1;32m    711\u001b[0m         args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkeywords}\n\u001b[0;32m--> 712\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/awswrangler/_utils.py:179\u001b[0m, in \u001b[0;36mvalidate_kwargs.<locals>.decorator.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m condition_fn() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(passed_unsupported_kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mInvalidArgument(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(passed_unsupported_kwargs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/awswrangler/athena/_read.py:1083\u001b[0m, in \u001b[0;36mread_sql_query\u001b[0;34m(sql, database, ctas_approach, unload_approach, ctas_parameters, unload_parameters, categories, chunksize, s3_output, workgroup, encryption, kms_key, keep_files, use_threads, boto3_session, client_request_token, athena_cache_settings, data_source, athena_query_wait_polling_delay, params, paramstyle, dtype_backend, s3_additional_kwargs, pyarrow_additional_kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m ctas_bucketing_info \u001b[38;5;241m=\u001b[39m ctas_parameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbucketing_info\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1081\u001b[0m ctas_write_compression \u001b[38;5;241m=\u001b[39m ctas_parameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1083\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_resolve_query_without_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43msql\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_source\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctas_approach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctas_approach\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43munload_approach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munload_approach\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m    \u001b[49m\u001b[43munload_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munload_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[43m    \u001b[49m\u001b[43ms3_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms3_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencryption\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencryption\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkms_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkms_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctas_database\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctas_database\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctas_temp_table_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctas_temp_table_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctas_bucketing_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctas_bucketing_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctas_write_compression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctas_write_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mathena_query_wait_polling_delay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mathena_query_wait_polling_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[43ms3_additional_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms3_additional_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mboto3_session\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mboto3_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpyarrow_additional_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpyarrow_additional_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecution_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecution_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_request_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_request_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/awswrangler/athena/_read.py:508\u001b[0m, in \u001b[0;36m_resolve_query_without_cache\u001b[0;34m(sql, database, data_source, ctas_approach, unload_approach, unload_parameters, categories, chunksize, s3_output, workgroup, encryption, kms_key, keep_files, ctas_database, ctas_temp_table_name, ctas_bucketing_info, ctas_write_compression, athena_query_wait_polling_delay, use_threads, s3_additional_kwargs, boto3_session, pyarrow_additional_kwargs, execution_params, dtype_backend, client_request_token)\u001b[0m\n\u001b[1;32m    506\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemp_table_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muuid\u001b[38;5;241m.\u001b[39muuid4()\u001b[38;5;241m.\u001b[39mhex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_resolve_query_without_cache_ctas\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43msql\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_source\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43ms3_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms3_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcategories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencryption\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencryption\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworkgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkms_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkms_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43malt_database\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctas_database\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctas_bucketing_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctas_bucketing_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctas_write_compression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctas_write_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mathena_query_wait_polling_delay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mathena_query_wait_polling_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43ms3_additional_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms3_additional_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mboto3_session\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mboto3_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpyarrow_additional_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpyarrow_additional_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexecution_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecution_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m     catalog\u001b[38;5;241m.\u001b[39mdelete_table_if_exists(database\u001b[38;5;241m=\u001b[39mctas_database \u001b[38;5;129;01mor\u001b[39;00m database, table\u001b[38;5;241m=\u001b[39mname, boto3_session\u001b[38;5;241m=\u001b[39mboto3_session)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/awswrangler/athena/_read.py:325\u001b[0m, in \u001b[0;36m_resolve_query_without_cache_ctas\u001b[0;34m(sql, database, data_source, s3_output, keep_files, chunksize, categories, encryption, workgroup, kms_key, alt_database, name, ctas_bucketing_info, ctas_write_compression, athena_query_wait_polling_delay, use_threads, s3_additional_kwargs, boto3_session, pyarrow_additional_kwargs, execution_params, dtype_backend)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_resolve_query_without_cache_ctas\u001b[39m(\n\u001b[1;32m    303\u001b[0m     sql: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    304\u001b[0m     database: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    323\u001b[0m     dtype_backend: Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy_nullable\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy_nullable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    324\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame \u001b[38;5;241m|\u001b[39m Iterator[pd\u001b[38;5;241m.\u001b[39mDataFrame]:\n\u001b[0;32m--> 325\u001b[0m     ctas_query_info: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m _QueryMetadata] \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_ctas_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m        \u001b[49m\u001b[43msql\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctas_table\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctas_database\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malt_database\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbucketing_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctas_bucketing_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_source\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43ms3_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms3_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworkgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencryption\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencryption\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrite_compression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctas_write_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkms_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkms_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mathena_query_wait_polling_delay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mathena_query_wait_polling_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mboto3_session\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mboto3_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecution_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparamstyle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mqmark\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m     fully_qualified_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mctas_query_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mctas_database\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mctas_query_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mctas_table\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    344\u001b[0m     ctas_query_metadata \u001b[38;5;241m=\u001b[39m cast(_QueryMetadata, ctas_query_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mctas_query_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/awswrangler/_config.py:712\u001b[0m, in \u001b[0;36mapply_configs.<locals>.wrapper\u001b[0;34m(*args_raw, **kwargs)\u001b[0m\n\u001b[1;32m    710\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m args[name]\n\u001b[1;32m    711\u001b[0m         args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkeywords}\n\u001b[0;32m--> 712\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/awswrangler/athena/_utils.py:869\u001b[0m, in \u001b[0;36mcreate_ctas_table\u001b[0;34m(sql, database, ctas_table, ctas_database, s3_output, storage_format, write_compression, partitioning_info, bucketing_info, field_delimiter, schema_only, workgroup, data_source, encryption, kms_key, categories, wait, athena_query_wait_polling_delay, execution_params, params, paramstyle, boto3_session)\u001b[0m\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn type is unknown\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m msg:\n\u001b[1;32m    865\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mInvalidArgumentValue(\n\u001b[1;32m    866\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease, don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt leave undefined columns types in your query. You can cast to ensure it. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(E.g. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSELECT CAST(NULL AS INTEGER) AS MY_COL, ...\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    868\u001b[0m             )\n\u001b[0;32m--> 869\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ex\n\u001b[1;32m    870\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mctas_query_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m query_execution_id\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/awswrangler/athena/_utils.py:847\u001b[0m, in \u001b[0;36mcreate_ctas_table\u001b[0;34m(sql, database, ctas_table, ctas_database, s3_output, storage_format, write_compression, partitioning_info, bucketing_info, field_delimiter, schema_only, workgroup, data_source, encryption, kms_key, categories, wait, athena_query_wait_polling_delay, execution_params, params, paramstyle, boto3_session)\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[1;32m    846\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 847\u001b[0m         response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mctas_query_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43m_get_query_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m            \u001b[49m\u001b[43mquery_execution_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_execution_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m            \u001b[49m\u001b[43mboto3_session\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mboto3_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcategories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata_cache_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_cache_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m            \u001b[49m\u001b[43mathena_query_wait_polling_delay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mathena_query_wait_polling_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mQueryFailed \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m    855\u001b[0m         msg: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(ex)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/awswrangler/athena/_utils.py:233\u001b[0m, in \u001b[0;36m_get_query_metadata\u001b[0;34m(query_execution_id, boto3_session, categories, query_execution_payload, metadata_cache_manager, athena_query_wait_polling_delay, execution_params, dtype_backend)\u001b[0m\n\u001b[1;32m    229\u001b[0m     _query_execution_payload \u001b[38;5;241m=\u001b[39m query_execution_payload\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m     _query_execution_payload \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryExecutionTypeDef\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 233\u001b[0m         \u001b[43m_executions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m            \u001b[49m\u001b[43mquery_execution_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_execution_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m            \u001b[49m\u001b[43mboto3_session\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mboto3_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m            \u001b[49m\u001b[43mathena_query_wait_polling_delay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mathena_query_wait_polling_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    238\u001b[0m     )\n\u001b[1;32m    239\u001b[0m cols_types: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m get_query_columns_types(\n\u001b[1;32m    240\u001b[0m     query_execution_id\u001b[38;5;241m=\u001b[39mquery_execution_id, boto3_session\u001b[38;5;241m=\u001b[39mboto3_session\n\u001b[1;32m    241\u001b[0m )\n\u001b[1;32m    242\u001b[0m _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCasting query column types: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, cols_types)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/awswrangler/_config.py:712\u001b[0m, in \u001b[0;36mapply_configs.<locals>.wrapper\u001b[0;34m(*args_raw, **kwargs)\u001b[0m\n\u001b[1;32m    710\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m args[name]\n\u001b[1;32m    711\u001b[0m         args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkeywords}\n\u001b[0;32m--> 712\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/awswrangler/athena/_executions.py:230\u001b[0m, in \u001b[0;36mwait_query\u001b[0;34m(query_execution_id, boto3_session, athena_query_wait_polling_delay)\u001b[0m\n\u001b[1;32m    228\u001b[0m _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery state change reason: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatus\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStateChangeReason\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFAILED\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 230\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mQueryFailed(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatus\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStateChangeReason\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANCELLED\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mQueryCancelled(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatus\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStateChangeReason\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mQueryFailed\u001b[0m: COLUMN_NOT_FOUND: line 8:13: Column 'well name' cannot be resolved or requester is not authorized to access requested resources. You may need to manually clean the data at location 's3://aws-athena-query-results-479837311121-us-east-1/tables/380e8c7b-645b-4c5a-8282-18d38e2bfac9' before retrying. Athena will not delete data in your account."
     ]
    }
   ],
   "source": [
    "# Create the Glue database if it doesn't exist\n",
    "wr.catalog.create_database(\n",
    "    name=\"petrophysics_db\",\n",
    "    description=\"Database for petrophysics data\",\n",
    "    exist_ok=True\n",
    ")\n",
    "\n",
    "# Delete the table if it already exists\n",
    "wr.catalog.delete_table_if_exists(database=\"petrophysics_db\", table=\"well_logs\")\n",
    "\n",
    "# Create Glue catalog table\n",
    "wr.catalog.create_csv_table(\n",
    "    database=\"petrophysics_db\",\n",
    "    table=\"well_logs\",\n",
    "    path=s3_input_path,\n",
    "    columns_types={\n",
    "        \"Facies\": \"string\",\n",
    "        \"Formation\": \"string\",\n",
    "        \"Well Name\": \"string\",\n",
    "        \"Depth\": \"float\",\n",
    "        \"GR\": \"float\",\n",
    "        \"ILD_log10\": \"float\",\n",
    "        \"DeltaPHI\": \"float\",\n",
    "        \"PHIND\": \"float\",\n",
    "        \"PE\": \"float\",\n",
    "        \"NM_M\": \"int\",\n",
    "        \"RELPOS\": \"float\"\n",
    "    },\n",
    "    partitions_types={},\n",
    ")\n",
    "\n",
    "# Query data using Athena\n",
    "df = wr.athena.read_sql_query(\n",
    "    sql=\"\"\"\n",
    "    SELECT * FROM petrophysics_db.well_logs\n",
    "    WHERE \"Well Name\" NOT IN ('SHRIMPLIN', 'SHANKLE')\n",
    "    \"\"\",\n",
    "    database=\"petrophysics_db\"\n",
    ")\n",
    "\n",
    "# Prepare features and target\n",
    "features = ['GR', 'ILD_log10', 'DeltaPHI', 'PHIND', 'PE', 'NM_M', 'RELPOS']\n",
    "target = 'Facies'\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save processed data to S3\n",
    "wr.s3.to_parquet(\n",
    "    df=pd.concat([X_train, y_train], axis=1),\n",
    "    path=f\"s3://{bucket}/{prefix}/processed_data/train/\",\n",
    "    dataset=True\n",
    ")\n",
    "\n",
    "wr.s3.to_parquet(\n",
    "    df=pd.concat([X_test, y_test], axis=1),\n",
    "    path=f\"s3://{bucket}/{prefix}/processed_data/test/\",\n",
    "    dataset=True\n",
    ")\n",
    "\n",
    "print(\"Data preparation completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c9b021-0cec-4029-a0e4-7e0c0fca8934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Parquet data from S3...\n",
      "Shape of df: (4149, 11)\n",
      "                Facies Formation Well_Name   Depth     GR  ILD_log10  \\\n",
      "0  Nonmarine sandstone     A1 SH   SHANKLE  2785.0  58.63      0.444   \n",
      "1  Nonmarine sandstone     A1 SH   SHANKLE  2785.5  59.16      0.413   \n",
      "2  Nonmarine sandstone     A1 SH   SHANKLE  2786.0  52.86      0.378   \n",
      "3  Nonmarine sandstone     A1 SH   SHANKLE  2786.5  54.32      0.344   \n",
      "4  Nonmarine sandstone     A1 SH   SHANKLE  2787.0  53.27      0.320   \n",
      "\n",
      "   DeltaPHI   PHIND   PE  NM_M  RELPOS  \n",
      "0       4.4  12.440  2.7     1   0.661  \n",
      "1       3.7  13.640  2.6     1   0.645  \n",
      "2       2.5  14.490  2.6     1   0.629  \n",
      "3       1.1  15.085  2.6     1   0.613  \n",
      "4      -0.3  14.985  2.7     1   0.597  \n",
      "\n",
      "Creating/Updating Athena table...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'awswrangler.catalog' has no attribute 'get_parquet_metadata'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 28\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# 2. Create or update the Athena table\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCreating/Updating Athena table...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m wr\u001b[38;5;241m.\u001b[39mcatalog\u001b[38;5;241m.\u001b[39mcreate_parquet_table(\n\u001b[1;32m     25\u001b[0m     database\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpetrophysics_db\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     26\u001b[0m     table\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwell_logs\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m     path\u001b[38;5;241m=\u001b[39ms3_parquet_path,\n\u001b[0;32m---> 28\u001b[0m     columns_types\u001b[38;5;241m=\u001b[39m\u001b[43mwr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcatalog\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parquet_metadata\u001b[49m(s3_parquet_path)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns_types\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     29\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# 3. Query data using Athena\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mQuerying data using Athena...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'awswrangler.catalog' has no attribute 'get_parquet_metadata'"
     ]
    }
   ],
   "source": [
    "import awswrangler as wr\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set up S3 client and bucket information\n",
    "session = boto3.Session()\n",
    "bucket = \"sagemaker-us-east-1-479837311121\"\n",
    "prefix = 'sagemaker/petrophysics-facies-classification'\n",
    "\n",
    "\n",
    "# S3 path for the Parquet file\n",
    "s3_parquet_path = f\"s3://{bucket}/{prefix}/parquet_data/welllog_data.parquet\"\n",
    "\n",
    "# 1. Read the Parquet data\n",
    "print(\"Reading Parquet data from S3...\")\n",
    "df = wr.s3.read_parquet(path=s3_parquet_path)\n",
    "\n",
    "print(f\"Shape of df: {df.shape}\")\n",
    "print(df.head())\n",
    "\n",
    "# 2. Create or update the Athena table\n",
    "print(\"\\nCreating/Updating Athena table...\")\n",
    "wr.catalog.create_parquet_table(\n",
    "    database=\"petrophysics_db\",\n",
    "    table=\"well_logs\",\n",
    "    path=s3_parquet_path,\n",
    "    columns_types=wr.catalog.get_parquet_metadata(s3_parquet_path)[\"columns_types\"],\n",
    "    mode=\"overwrite\"\n",
    ")\n",
    "\n",
    "# 3. Query data using Athena\n",
    "print(\"\\nQuerying data using Athena...\")\n",
    "df = wr.athena.read_sql_query(\n",
    "    sql=\"\"\"\n",
    "    SELECT * FROM petrophysics_db.well_logs\n",
    "    WHERE \"Well Name\" NOT IN ('SHRIMPLIN', 'SHANKLE')\n",
    "    \"\"\",\n",
    "    database=\"petrophysics_db\"\n",
    ")\n",
    "\n",
    "print(f\"Shape of df after Athena query: {df.shape}\")\n",
    "print(df.head())\n",
    "\n",
    "# 4. Prepare features and target\n",
    "print(\"\\nPreparing features and target...\")\n",
    "features = ['GR', 'ILD_log10', 'DeltaPHI', 'PHIND', 'PE', 'NM_M', 'RELPOS']\n",
    "target = 'Facies'\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of y: {y.shape}\")\n",
    "\n",
    "# 5. Split data\n",
    "print(\"\\nSplitting data...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}\")\n",
    "\n",
    "# 6. Save processed data to S3\n",
    "print(\"\\nSaving processed data to S3...\")\n",
    "wr.s3.to_parquet(\n",
    "    df=pd.concat([X_train, y_train], axis=1),\n",
    "    path=f\"s3://{bucket}/{prefix}/processed_data/train/\",\n",
    "    dataset=True\n",
    ")\n",
    "\n",
    "wr.s3.to_parquet(\n",
    "    df=pd.concat([X_test, y_test], axis=1),\n",
    "    path=f\"s3://{bucket}/{prefix}/processed_data/test/\",\n",
    "    dataset=True\n",
    ")\n",
    "\n",
    "print(\"Data preparation completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6e0630-3c80-4db0-b3ee-5cd71256719d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df after Athena query: (4149, 11)\n",
      "                Facies Formation Well_Name   Depth     GR  ILD_log10  \\\n",
      "0  Nonmarine sandstone     A1 SH   SHANKLE  2785.0  58.63      0.444   \n",
      "1  Nonmarine sandstone     A1 SH   SHANKLE  2785.5  59.16      0.413   \n",
      "2  Nonmarine sandstone     A1 SH   SHANKLE  2786.0  52.86      0.378   \n",
      "3  Nonmarine sandstone     A1 SH   SHANKLE  2786.5  54.32      0.344   \n",
      "4  Nonmarine sandstone     A1 SH   SHANKLE  2787.0  53.27      0.320   \n",
      "\n",
      "   DeltaPHI   PHIND   PE  NM_M  RELPOS  \n",
      "0       4.4  12.440  2.7     1   0.661  \n",
      "1       3.7  13.640  2.6     1   0.645  \n",
      "2       2.5  14.490  2.6     1   0.629  \n",
      "3       1.1  15.085  2.6     1   0.613  \n",
      "4      -0.3  14.985  2.7     1   0.597  \n",
      "\n",
      "Preparing features and target...\n",
      "Shape of X: (4149, 7)\n",
      "Shape of y: (4149,)\n",
      "\n",
      "Splitting data...\n",
      "Shape of X_train: (3319, 7)\n",
      "Shape of X_test: (830, 7)\n",
      "\n",
      "Saving processed data to S3...\n",
      "Data preparation completed successfully.\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "\n",
    "# 4. Prepare features and target\n",
    "print(\"\\nPreparing features and target...\")\n",
    "features = ['GR', 'ILD_log10', 'DeltaPHI', 'PHIND', 'PE', 'NM_M', 'RELPOS']\n",
    "target = 'Facies'\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of y: {y.shape}\")\n",
    "\n",
    "# 5. Split data\n",
    "print(\"\\nSplitting data...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}\")\n",
    "\n",
    "# 6. Save processed data to S3\n",
    "print(\"\\nSaving processed data to S3...\")\n",
    "wr.s3.to_parquet(\n",
    "    df=pd.concat([X_train, y_train], axis=1),\n",
    "    path=f\"s3://{bucket}/{prefix}/processed_data/train/\",\n",
    "    dataset=True\n",
    ")\n",
    "\n",
    "wr.s3.to_parquet(\n",
    "    df=pd.concat([X_test, y_test], axis=1),\n",
    "    path=f\"s3://{bucket}/{prefix}/processed_data/test/\",\n",
    "    dataset=True\n",
    ")\n",
    "\n",
    "print(\"Data preparation completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8601a66-5e2b-40cd-aab1-ef1eaaaebb1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Facies</th>\n",
       "      <th>Formation</th>\n",
       "      <th>Well_Name</th>\n",
       "      <th>Depth</th>\n",
       "      <th>GR</th>\n",
       "      <th>ILD_log10</th>\n",
       "      <th>DeltaPHI</th>\n",
       "      <th>PHIND</th>\n",
       "      <th>PE</th>\n",
       "      <th>NM_M</th>\n",
       "      <th>RELPOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nonmarine sandstone</td>\n",
       "      <td>A1 SH</td>\n",
       "      <td>SHANKLE</td>\n",
       "      <td>2785.0</td>\n",
       "      <td>58.63</td>\n",
       "      <td>0.444</td>\n",
       "      <td>4.4</td>\n",
       "      <td>12.440</td>\n",
       "      <td>2.7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nonmarine sandstone</td>\n",
       "      <td>A1 SH</td>\n",
       "      <td>SHANKLE</td>\n",
       "      <td>2785.5</td>\n",
       "      <td>59.16</td>\n",
       "      <td>0.413</td>\n",
       "      <td>3.7</td>\n",
       "      <td>13.640</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nonmarine sandstone</td>\n",
       "      <td>A1 SH</td>\n",
       "      <td>SHANKLE</td>\n",
       "      <td>2786.0</td>\n",
       "      <td>52.86</td>\n",
       "      <td>0.378</td>\n",
       "      <td>2.5</td>\n",
       "      <td>14.490</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nonmarine sandstone</td>\n",
       "      <td>A1 SH</td>\n",
       "      <td>SHANKLE</td>\n",
       "      <td>2786.5</td>\n",
       "      <td>54.32</td>\n",
       "      <td>0.344</td>\n",
       "      <td>1.1</td>\n",
       "      <td>15.085</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nonmarine sandstone</td>\n",
       "      <td>A1 SH</td>\n",
       "      <td>SHANKLE</td>\n",
       "      <td>2787.0</td>\n",
       "      <td>53.27</td>\n",
       "      <td>0.320</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>14.985</td>\n",
       "      <td>2.7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Facies Formation Well_Name   Depth     GR  ILD_log10  \\\n",
       "0  Nonmarine sandstone     A1 SH   SHANKLE  2785.0  58.63      0.444   \n",
       "1  Nonmarine sandstone     A1 SH   SHANKLE  2785.5  59.16      0.413   \n",
       "2  Nonmarine sandstone     A1 SH   SHANKLE  2786.0  52.86      0.378   \n",
       "3  Nonmarine sandstone     A1 SH   SHANKLE  2786.5  54.32      0.344   \n",
       "4  Nonmarine sandstone     A1 SH   SHANKLE  2787.0  53.27      0.320   \n",
       "\n",
       "   DeltaPHI   PHIND   PE  NM_M  RELPOS  \n",
       "0       4.4  12.440  2.7     1   0.661  \n",
       "1       3.7  13.640  2.6     1   0.645  \n",
       "2       2.5  14.490  2.6     1   0.629  \n",
       "3       1.1  15.085  2.6     1   0.613  \n",
       "4      -0.3  14.985  2.7     1   0.597  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b19de5-44f0-417e-a8d4-690f409023e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Glue catalog table\n",
    "wr.catalog.create_csv_table(  # Note: changed from create_parquet_table to create_csv_table\n",
    "    database=\"petrophysics_db\",\n",
    "    table=\"well_logs\",\n",
    "    path=s3_input_path,\n",
    "    columns_types={\n",
    "        \"Well Name\": \"string\",  # Changed from \"Well Name\" to \"well name\"\n",
    "        \"Depth\": \"float\",\n",
    "        \"GR\": \"float\",\n",
    "        \"ILD_log10\": \"float\",\n",
    "        \"DeltaPHI\": \"float\",\n",
    "        \"PHIND\": \"float\",\n",
    "        \"PE\": \"float\",\n",
    "        \"NM_M\": \"int\",\n",
    "        \"RELPOS\": \"float\",\n",
    "        \"Facies\": \"string\"\n",
    "    },\n",
    "    partitions_types={},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a33422-6f3d-4bd6-8eab-3f138f3e301b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"   \n",
    "Model Development\n",
    "We'll use PyTorch to create a deep learning model:\n",
    "\"\"\"\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FaciesClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FaciesClassifier, self).__init__()\n",
    "        self.layer_1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.layer_2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer_out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "        return x\n",
    "\n",
    "# Define model architecture\n",
    "input_dim = len(features)\n",
    "hidden_dim = 64\n",
    "output_dim = len(y.unique())\n",
    "\n",
    "model = FaciesClassifier(input_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f89f2d0-28ba-47a5-af1a-47ac26b2f08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.tuner import HyperparameterTuner, IntegerParameter, ContinuousParameter\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "# Set up the SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Define your S3 bucket and prefix\n",
    "bucket = \"sagemaker-us-east-1-479837311121\"\n",
    "prefix = \"sagemaker/petrophysics-facies-classification\"\n",
    "\n",
    "\n",
    "# Define hyperparameter ranges\n",
    "hyperparameter_ranges = {\n",
    "    \"learning_rate\": ContinuousParameter(0.001, 0.1),\n",
    "    \"batch_size\": IntegerParameter(32, 256),\n",
    "    \"hidden_dim\": IntegerParameter(32, 256),\n",
    "}\n",
    "\n",
    "# Create PyTorch estimator\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    framework_version=\"1.9.1\",\n",
    "    py_version=\"py3\",\n",
    "    hyperparameters={\n",
    "        \"epochs\": 50,\n",
    "    },\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# Create hyperparameter tuner\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator,\n",
    "    objective_metric_name=\"validation_accuracy\",\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    metric_definitions=[\n",
    "        {\"Name\": \"validation_accuracy\", \"Regex\": \"Validation accuracy: ([0-9\\\\.]+)\"}\n",
    "    ],\n",
    "    max_jobs=20,\n",
    "    max_parallel_jobs=3,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e1dc78-d715-4673-96d2-9a1ecacfccf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................................*\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for HyperParameterTuning job pytorch-training-250109-1757: Failed. Reason: No training job succeeded after 5 attempts. For additional details, please take a look at the training job failures by listing training jobs for the hyperparameter tuning job.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 47\u001b[0m\n\u001b[1;32m     35\u001b[0m tuner \u001b[38;5;241m=\u001b[39m HyperparameterTuner(\n\u001b[1;32m     36\u001b[0m     estimator,\n\u001b[1;32m     37\u001b[0m     objective_metric_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     max_parallel_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Start hyperparameter tuning job\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms3://\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbucket\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/processed_data/train\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms3://\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbucket\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/processed_data/test\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Get best training job\u001b[39;00m\n\u001b[1;32m     51\u001b[0m best_training_job \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mbest_training_job()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/workflow/pipeline_context.py:346\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/tuner.py:1044\u001b[0m, in \u001b[0;36mHyperparameterTuner.fit\u001b[0;34m(self, inputs, job_name, include_cls_metadata, estimator_kwargs, wait, **kwargs)\u001b[0m\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_with_estimator_dict(inputs, job_name, include_cls_metadata, estimator_kwargs)\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 1044\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_tuning_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/tuner.py:2353\u001b[0m, in \u001b[0;36m_TuningJob.wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   2352\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Placeholder docstring.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2353\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_for_tuning_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/session.py:5273\u001b[0m, in \u001b[0;36mSession.wait_for_tuning_job\u001b[0;34m(self, job, poll)\u001b[0m\n\u001b[1;32m   5259\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wait for an Amazon SageMaker hyperparameter tuning job to complete.\u001b[39;00m\n\u001b[1;32m   5260\u001b[0m \n\u001b[1;32m   5261\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5270\u001b[0m \u001b[38;5;124;03m    exceptions.UnexpectedStatusException: If the hyperparameter tuning job fails.\u001b[39;00m\n\u001b[1;32m   5271\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5272\u001b[0m desc \u001b[38;5;241m=\u001b[39m _wait_until(\u001b[38;5;28;01mlambda\u001b[39;00m: _tuning_job_status(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_client, job), poll)\n\u001b[0;32m-> 5273\u001b[0m \u001b[43m_check_job_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHyperParameterTuningJobStatus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m desc\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/session.py:8508\u001b[0m, in \u001b[0;36m_check_job_status\u001b[0;34m(job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   8502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCapacityError\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(reason):\n\u001b[1;32m   8503\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCapacityError(\n\u001b[1;32m   8504\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   8505\u001b[0m         allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   8506\u001b[0m         actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   8507\u001b[0m     )\n\u001b[0;32m-> 8508\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   8509\u001b[0m     message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   8510\u001b[0m     allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   8511\u001b[0m     actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   8512\u001b[0m )\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for HyperParameterTuning job pytorch-training-250109-1757: Failed. Reason: No training job succeeded after 5 attempts. For additional details, please take a look at the training job failures by listing training jobs for the hyperparameter tuning job."
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.tuner import HyperparameterTuner, IntegerParameter, ContinuousParameter\n",
    "\n",
    "# Set up the SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Define your S3 bucket and prefix\n",
    "bucket = \"sagemaker-us-east-1-479837311121\"\n",
    "prefix = \"sagemaker/petrophysics-facies-classification\"\n",
    "\n",
    "# Define hyperparameter ranges\n",
    "hyperparameter_ranges = {\n",
    "    \"learning_rate\": ContinuousParameter(0.001, 0.1),\n",
    "    \"batch_size\": IntegerParameter(32, 256),\n",
    "    \"hidden_dim\": IntegerParameter(32, 256),\n",
    "}\n",
    "\n",
    "# Create PyTorch estimator\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    framework_version=\"1.9.1\",\n",
    "    py_version=\"py38\",\n",
    "    hyperparameters={\n",
    "        \"epochs\": 50,\n",
    "    },\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# Create hyperparameter tuner\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator,\n",
    "    objective_metric_name=\"validation_accuracy\",\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    metric_definitions=[\n",
    "        {\"Name\": \"validation_accuracy\", \"Regex\": \"Validation accuracy: ([0-9\\\\.]+)\"}\n",
    "    ],\n",
    "    max_jobs=20,\n",
    "    max_parallel_jobs=3,\n",
    ")\n",
    "\n",
    "# Start hyperparameter tuning job\n",
    "tuner.fit({\"train\": f\"s3://{bucket}/{prefix}/processed_data/train\",\n",
    "           \"test\": f\"s3://{bucket}/{prefix}/processed_data/test\"})\n",
    "\n",
    "# Get best training job\n",
    "best_training_job = tuner.best_training_job()\n",
    "print(f\"Best training job: {best_training_job}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274fcedd-cd4a-42ad-bed8-f6ef1e58a66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2025-01-09-18-17-17-647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-09 18:17:21 Starting - Starting the training job...\n",
      "2025-01-09 18:17:34 Starting - Preparing the instances for training...\n",
      "2025-01-09 18:18:19 Downloading - Downloading the training image......\n",
      "2025-01-09 18:19:05 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2025-01-09 18:19:15,270 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2025-01-09 18:19:15,272 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2025-01-09 18:19:15,281 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2025-01-09 18:19:15,283 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2025-01-09 18:19:15,442 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2025-01-09 18:19:15,453 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2025-01-09 18:19:15,462 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2025-01-09 18:19:15,471 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 50\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2025-01-09-18-17-17-647\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-479837311121/pytorch-training-2025-01-09-18-17-17-647/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":50}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-479837311121/pytorch-training-2025-01-09-18-17-17-647/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":50},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2025-01-09-18-17-17-647\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-479837311121/pytorch-training-2025-01-09-18-17-17-647/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"50\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=50\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 train.py --epochs 50\u001b[0m\n",
      "\u001b[34mContents of input directory:\u001b[0m\n",
      "\u001b[34m['41cf0ea6b4fc4df98035e30e51cf8610.snappy.parquet']\u001b[0m\n",
      "\u001b[34mContents of test directory:\u001b[0m\n",
      "\u001b[34m['6ce97282318a48f8aaa7a394487ebb7f.snappy.parquet']\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"train.py\", line 99, in <module>\u001b[0m\n",
      "\u001b[34mtrain(args)\n",
      "  File \"train.py\", line 36, in train\n",
      "    train_data = pd.read_parquet(os.path.join(args.train, 'train.parquet'))\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pandas/io/parquet.py\", line 493, in read_parquet\u001b[0m\n",
      "\u001b[34mreturn impl.read(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pandas/io/parquet.py\", line 233, in read\u001b[0m\n",
      "\u001b[34mpath_or_handle, handles, kwargs[\"filesystem\"] = _get_path_or_handle(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pandas/io/parquet.py\", line 102, in _get_path_or_handle\n",
      "    handles = get_handle(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pandas/io/common.py\", line 798, in get_handle\n",
      "    handle = open(handle, ioargs.mode)\u001b[0m\n",
      "\u001b[34mFileNotFoundError: [Errno 2] No such file or directory: '/opt/ml/input/data/train/train.parquet'\u001b[0m\n",
      "\u001b[34m2025-01-09 18:19:16,314 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2025-01-09 18:19:16,314 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 1 from exiting process.\u001b[0m\n",
      "\u001b[34m2025-01-09 18:19:16,315 sagemaker-training-toolkit ERROR    Reporting training FAILURE\u001b[0m\n",
      "\u001b[34m2025-01-09 18:19:16,315 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mExitCode 1\u001b[0m\n",
      "\u001b[34mErrorMessage \"FileNotFoundError: [Errno 2] No such file or directory: '/opt/ml/input/data/train/train.parquet'\u001b[0m\n",
      "\u001b[34m\"\u001b[0m\n",
      "\u001b[34mCommand \"/opt/conda/bin/python3.8 train.py --epochs 50\"\u001b[0m\n",
      "\u001b[34m2025-01-09 18:19:16,315 sagemaker-training-toolkit ERROR    Encountered exit_code 1\u001b[0m\n",
      "\n",
      "2025-01-09 18:19:39 Uploading - Uploading generated training model\n",
      "2025-01-09 18:19:39 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job pytorch-training-2025-01-09-18-17-17-647: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"FileNotFoundError: [Errno 2] No such file or directory: '/opt/ml/input/data/train/train.parquet'\n\"\nCommand \"/opt/conda/bin/python3.8 train.py --epochs 50\", exit code: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms3://\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbucket\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/processed_data/train\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m               \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms3://\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbucket\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/processed_data/test\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/workflow/pipeline_context.py:346\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/estimator.py:1350\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job)\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 1350\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_training_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/estimator.py:2720\u001b[0m, in \u001b[0;36m_TrainingJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2718\u001b[0m \u001b[38;5;66;03m# If logs are requested, call logs_for_jobs.\u001b[39;00m\n\u001b[1;32m   2719\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 2720\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2721\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2722\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mwait_for_job(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_name)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/session.py:5853\u001b[0m, in \u001b[0;36mSession.logs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   5832\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlogs_for_job\u001b[39m(\u001b[38;5;28mself\u001b[39m, job_name, wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, poll\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, log_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll\u001b[39m\u001b[38;5;124m\"\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   5833\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Display logs for a given training job, optionally tailing them until job is complete.\u001b[39;00m\n\u001b[1;32m   5834\u001b[0m \n\u001b[1;32m   5835\u001b[0m \u001b[38;5;124;03m    If the output is a tty or a Jupyter cell, it will be color-coded\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5851\u001b[0m \u001b[38;5;124;03m        exceptions.UnexpectedStatusException: If waiting and the training job fails.\u001b[39;00m\n\u001b[1;32m   5852\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5853\u001b[0m     \u001b[43m_logs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/session.py:8455\u001b[0m, in \u001b[0;36m_logs_for_job\u001b[0;34m(sagemaker_session, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   8452\u001b[0m             last_profiler_rule_statuses \u001b[38;5;241m=\u001b[39m profiler_rule_statuses\n\u001b[1;32m   8454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 8455\u001b[0m     \u001b[43m_check_job_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrainingJobStatus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   8456\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dot:\n\u001b[1;32m   8457\u001b[0m         \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/session.py:8508\u001b[0m, in \u001b[0;36m_check_job_status\u001b[0;34m(job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   8502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCapacityError\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(reason):\n\u001b[1;32m   8503\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCapacityError(\n\u001b[1;32m   8504\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   8505\u001b[0m         allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   8506\u001b[0m         actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   8507\u001b[0m     )\n\u001b[0;32m-> 8508\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   8509\u001b[0m     message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   8510\u001b[0m     allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   8511\u001b[0m     actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   8512\u001b[0m )\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job pytorch-training-2025-01-09-18-17-17-647: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"FileNotFoundError: [Errno 2] No such file or directory: '/opt/ml/input/data/train/train.parquet'\n\"\nCommand \"/opt/conda/bin/python3.8 train.py --epochs 50\", exit code: 1"
     ]
    }
   ],
   "source": [
    "estimator.fit({\"train\": f\"s3://{bucket}/{prefix}/processed_data/train\",\n",
    "               \"test\": f\"s3://{bucket}/{prefix}/processed_data/test\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8001fc-98c9-4bc1-b0a6-5d90edb44f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"    \n",
    "Monitoring and MLOps\n",
    "Set up model monitoring and implement MLOps practices:\n",
    "\"\"\"\n",
    "    \n",
    "from sagemaker.model_monitor import DataCaptureConfig, ModelMonitor\n",
    "\n",
    "# Enable data capture\n",
    "data_capture_config = DataCaptureConfig(\n",
    "    enable_capture=True,\n",
    "    sampling_percentage=100,\n",
    "    destination_s3_uri=f\"s3://{bucket}/{prefix}/data_capture\"\n",
    ")\n",
    "\n",
    "# Create model monitor\n",
    "model_monitor = ModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "# Set up monitoring schedule\n",
    "model_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name='facies-classification-monitor',\n",
    "    endpoint_input=predictor.endpoint,\n",
    "    statistics=model_monitor.baseline_statistics(),\n",
    "    constraints=model_monitor.suggested_constraints(),\n",
    "    schedule_cron_expression='cron(0 * ? * * *)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc9053d-5130-4418-8631-0d96a4f6f425",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"    \n",
    "Cleanup\n",
    "Clean up resources when you're done:\n",
    "\"\"\"\n",
    "    \n",
    "# Delete endpoint\n",
    "predictor.delete_endpoint()\n",
    "\n",
    "# Delete model monitor schedule\n",
    "model_monitor.delete_monitoring_schedule()\n",
    "\n",
    "# Delete S3 objects\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket(bucket)\n",
    "bucket.objects.filter(Prefix=prefix).delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4587e669-9650-4c2a-890b-82508148d49f",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "    \n",
    "This updated version incorporates modern AWS services, improves the ML pipeline with deep learning and hyperparameter tuning, adds data preparation using Glue and Athena, implements model monitoring, and follows MLOps best practices. Remember to create appropriate IAM roles, encrypt data, and follow AWS security best practices throughout the implementation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tslearn",
   "language": "python",
   "name": "tslearn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
